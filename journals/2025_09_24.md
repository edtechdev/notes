- [PsyArXiv Preprints | Quantifying Human-AI Synergy](https://osf.io/preprints/psyarxiv/vbkmt_v1)
	- [[Human-computer interaction]], [[Artificial intelligence]], [[Productivity]], [[AI literacy]], [[Item response theory]]
- [Why Donâ€™t We Ditch the Textbook? | TeachOnline](https://teachonline.ca/tools-trends/articles/why-dont-we-ditch-the-textbook/)
	- [[Interactive textbook]], [[Textbook]], [[Artificial intelligence in education]]
- [Why accessibility might be AIâ€™s biggest breakthrough - Ars Technica](https://arstechnica.com/information-technology/2025/09/study-finds-neurodiverse-workers-more-satisfied-with-ai-assistants/)
  collapsed:: true
	- [[Accessibility]], [[Artificial intelligence]], [[Artificial intelligence in education]]
- [AI is impressive because weâ€™ve failed at semantic web and personal computing | exotext](https://rakhim.exotext.com/ai-is-impressive-because-we-ve-failed-at-semantic-web-and-personal-computing)
	- [[Semantic web]], [[LLM]], [[Search engine]]
- [GitHub - mtdvio/every-programmer-should-know: A collection of (mostly) technical things every software developer should know about](https://github.com/mtdvio/every-programmer-should-know)
	- [[Computer science education]], [[programming]]
- [Beyond Tool or Threat: GenAI and the Challenge It Poses to Higher Education | EDUCAUSE Review](https://er.educause.edu/articles/2025/9/beyond-tool-or-threat-genai-and-the-challenge-it-poses-to-higher-education)
	- [[Artificial intelligence in education]]
- [Synthesis Tutor](https://www.synthesis.com/tutor)
	- [[Intelligent tutoring system]], [[Multimodal AI]]
- [Enacting assessment reform in a time of artificial intelligence | Tertiary Education Quality and Standards Agency](https://www.teqsa.gov.au/guides-resources/resources/corporate-publications/enacting-assessment-reform-time-artificial-intelligence)
	- [[Assessment]], [[Artificial intelligence in education]], [[Higher education reform]]
- [weaviate/weaviate: Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native databaseâ€‹.](https://github.com/weaviate/weaviate)
	- [[Vector database]], [[AIOps]], [[Fine tuning]], [[huggingface]]
	- [Femke Plantinga (@femke_plantinga): "Should you fine-tune your embedding model? (Spoiler: probably not ğ˜ºğ˜¦ğ˜µ) ğ˜‰ğ˜¦ğ˜§ğ˜°ğ˜³ğ˜¦ jumping into fine-tuning, ask yourself: is your retrieval pipeline actually failing because of domain-specific knowledge gaps, or could it be something simpler? Here's what to check first: â€¢ Are you using the right chunking technique? Maybe late chunking would help â€¢ Do you need exact keyword matches? Try hybrid search instead â€¢ Is your current model just not capturing enough contextual nuances? A model with more dimensions might solve this ğ—œğ—³ ğ˜†ğ—¼ğ˜‚ğ—¿ ğ—ºğ—¼ğ—±ğ—²ğ—¹ ğ—¶ğ˜€ ğ—¼ğ—»ğ—¹ğ˜† ğ—³ğ—®ğ—¶ğ—¹ğ—¶ğ—»ğ—´ ğ—¼ğ—» ğ—±ğ—¼ğ—ºğ—®ğ—¶ğ—»-ğ˜€ğ—½ğ—²ğ—°ğ—¶ğ—³ğ—¶ğ—° ğ˜€ğ—²ğ—ºğ—®ğ—»ğ˜ğ—¶ğ—° ğ—¿ğ—²ğ—¹ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€ğ—µğ—¶ğ—½ğ˜€, ğ˜ğ—µğ—²ğ—» ğ—³ğ—¶ğ—»ğ—²-ğ˜ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ—ºğ—¶ğ—´ğ—µğ˜ ğ—¯ğ—² ğ˜„ğ—¼ğ—¿ğ˜ğ—µ ğ—¶ğ˜. Fine-tuning embedding models works differently than fine-tuning LLMs. Instead of next-token prediction, it uses contrastive learning methods that adjust vector distances in embedding space. The core principle: given an anchor data point, which candidate is most similar? The optimization process pulls positive pairs closer together while pushing negative pairs apart. Popular loss functions include: â†’ ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—½ğ—¹ğ—² ğ—¡ğ—²ğ—´ğ—®ğ˜ğ—¶ğ˜ƒğ—²ğ˜€ ğ—¥ğ—®ğ—»ğ—¸ğ—¶ğ—»ğ—´ ğ—Ÿğ—¼ğ˜€ğ˜€: Simple text pairs, treats other batch examples as negatives â†’ ğ—§ğ—¿ğ—¶ğ—½ğ—¹ğ—²ğ˜ ğ—Ÿğ—¼ğ˜€ğ˜€: Requires careful curation of (anchor, positive, negative) triplets â†’ ğ—–ğ—¼ğ˜€ğ—¶ğ—»ğ—² ğ—˜ğ—ºğ—¯ğ—²ğ—±ğ—±ğ—¶ğ—»ğ—´ ğ—Ÿğ—¼ğ˜€ğ˜€: Uses similarity scores for graded relationships ğ—§ğ—µğ—² ğ—´ğ—¼ğ—¼ğ—± ğ—»ğ—²ğ˜„ğ˜€? Fine-tuning costs way less than pre-training - sometimes just a few dollars for simple tasks, and you can even run smaller models on Google Colab's free tier. ğ——ğ—®ğ˜ğ—®ğ˜€ğ—²ğ˜ ğ—¿ğ—²ğ—¾ğ˜‚ğ—¶ğ—¿ğ—²ğ—ºğ—²ğ—»ğ˜ğ˜€: Start with 1,000-5,000 high-quality samples for narrow domains, scale to 10,000+ for complex specialized terminology. Once you've fine-tuned a model and pushed it to @huggingface, you can use it directly with @weaviate_io through either the Hugging Face integration or AWS SageMaker. But remember - fine-tuning should be your last resort, not your first instinct. Often, the solution is simpler than you think. âœï¸Â Read the full guide: https://weaviate.io/blog/fine-tune-embedding-model?utm_source=channels&utm_medium=fp_social&utm_campaign=dev_education&utm_content=animated_diagram_post_680387995 ğŸ“•Â Notebook: How to deploy Embedding Models to Amazon SageMaker using new Hugging Face Embedding DLC: https://github.com/huggingface/notebooks/blob/main/sagemaker/31_deploy_embedding_models/sagemaker-notebook.ipynb ğŸ¤Â Weaviate Hugging Face integration: https://docs.weaviate.io/weaviate/model-providers/huggingface/embeddings?__hstc=13542376.55f267c7a964b4e79a8cfd0a6626b57a.1739181214502.1751296605789.1751389705606.102&__hssc=13542376.6.1757933987251&__hsfp=3440847475" | nitter](https://nitter.net/femke_plantinga/status/1970412925727777166#m)
	- [Why, When and How to Fine-Tune a Custom Embedding Model | Weaviate](https://weaviate.io/blog/fine-tune-embedding-model)
	- [Text Embeddings | Weaviate Documentation](https://docs.weaviate.io/weaviate/model-providers/huggingface/embeddings)
- [QwenLM/Qwen3-Omni: Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.](https://github.com/QwenLM/Qwen3-Omni)
	- [[Multimodal AI]], [[Open LLM]], [[huggingface]], [[Speech recognition]], [[Visual understanding]], [[Text-to-speech]]
	- [Qwen3-Omni - a Qwen Collection](https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe)
- [NearlyFreeSpeech.NET Web Hosting](https://www.nearlyfreespeech.net/)
	- [[Hosting]], [[Indieweb]]
	- [Corposphere escape velocity: CI/CD on my very specific indie tech stack | mostol.dev](https://www.mostol.dev/post/202509172307/)
	- [Disroot | Disroot.org](https://disroot.org/en)
- [New Report: Do Teachers Have What They Need?](https://app.e.gallup.com/e/es?s=831949997&e=4189459&elqTrackId=efd74c1a1b7a40299e524d6e5aa03bea&elq=fc90d58025ae486eb48d7f18ab6487a7&elqaid=15645&elqat=1&elqak=8AF5B15E1677546675A5E1D0632D64785F0191FB07361564E5A95235859BD1EE91B7)
	- [[Professional development]], [[Educational development]], [[Faculty learning community]]
- [GenAIAssessLearnToolkit | AAIEEC](https://www.aaieec.org/genaiassessmenttoolkit)
	- [[Assessment]], [[Artificial intelligence in education]]
- [Prompt Potential: A Pilot Assessment Of Using Generative Artificial Intelligence (Chatgpt-4) As A Tutor For Engineering And Maths by Sasha Nikolic, Ashley Heath, Bao Anh Vu, Scott Daniel, Armin Alimardani, Carolyn Sandison, Xiaoping Lu, Brad Stappenbelt, David Hastie :: SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5121997)
	- [[Prompt engineering]], [[Intelligent tutoring system]], [[Mathematics education]]
	- [Teach Me, Test Me: Using ChatGPT as Both Tutor and Student](https://www.aaieec.org/post/teach-me-test-me-using-chatgpt-as-both-tutor-and-student)
- [Startups technical guide: AI agents | Google Cloud](https://cloud.google.com/resources/content/building-ai-agents)
	- [[AI agents]], [[AIOps]]
-